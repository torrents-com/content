#!/usr/bin/env python

"""
Launch a spider.

You can pass additional parameters to the spider after the spider
name.

Example:
  smon add generic start_urls=http://bt-chat.com/,http://1337x.org/ max_links=4 max_depth=3
"""

import sys
import socket
import urllib2
from argparse import ArgumentParser, RawDescriptionHelpFormatter

from smon_utils import add


def main():
    parser = ArgumentParser(description=__doc__,
                            formatter_class=RawDescriptionHelpFormatter)
    parser.add_argument('--silent', action='store_true', help='be less verbose')
    parser.add_argument('spider', nargs=1, help='name of the spider to add')
    parser.add_argument('option', nargs='*', help='spider options')
    args = parser.parse_args()

    if not args.silent:
        print 'Scheduling spider %s ...' % args.spider[0]

    jid = add(args.spider[0], args.option)

    if not args.silent:
        print 'Done. Its job id is %s' % jid



if __name__ == '__main__':
    try:
        main()
    except socket.timeout as e:
        sys.exit('Error: timed out while connecting to web server. Try again?')
    except urllib2.URLError as e:
        sys.exit('Error: %s\nMaybe scrapy server is not running?' % e)
    except RuntimeError as e:
        sys.exit(e)
